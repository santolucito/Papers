\section{Introduction}
We enable programming by example that leverages user defined higher order functions.
There are two steps, preprocessing and user-level synthesis, which could be compared to compile time and runtime for the PBE engine.

In the runtime process, we will choose an appropriate candidate higher order function.
With that higher order function, we will need to synthesize its component function.
We then recursively apply out PBE engine.
This exactly the $\Lambda^2$ algorithm I think.

In the preprocessing step we synthesize refinement types for the user defined higher order functions to be used for determining which functions to apply to a given example set.
In order to synthesize the component functions at runtime, we will synthesize a subexample generation function in preprocessing.
We need to generate subexamples to be able to recursively apply the PBE engine.
In a fascinating and twisted recursion, the subexample generation function is also synthesized using the PBE engine. 

After the preprocessing step, from which we have obtained refinement types and subexample generation functions, we then move to the actual user-level synthesis.
We will generate refinement types for the given examples then use type checking to choose a good higher order function to explore.

\section{Preprocessing}
In preprocessing, we are generating hypotheses for all user defined higher order functions.
The hypotheses say something about the semantic meaning of hoe the function works.
We have refinement types which act as statements about the applicability of the function to example sets.
We have subexample generation functions for each higher order function that are statements about how the component function will behave.

\subsection{Refinement type generation}
Pull from a pool of template refinement types that fit the typeclass constraints. 
If we wanted to synthesize these, we probably could (though I'm not sure how or if it is useful at all).

\subsection{Subexample generation}
After choosing a higher order function to explore as a candidate for the user provided examples, will will need to also synthesize the component function.
For example if we have \textbf{map f list}, we need to also synthesize \textbf{f}.
We will rely on recursively calling our PBE engine on \textbf{f}, which means we need to have examples for \textbf{f}.
These examples can be extracted from the top level user provided examples, but that requires a subexample generation function for every higher order function.

We can generate as many examples for a higher order function as we want (using quickcheck's co/arbitrary classes).
We will also wrap the component function in a state monad that allows us to record its execution on all of the examples.

As an example, consider \textbf{map}.
Given the type of \textbf{map :: (a -> b) -> [a] -> [b]}, the subexample generation function will have type \textbf{:: [a] -> [b] -> [(a,b)]}.
We might generate the example input \textbf{(+1) [1,2,3]}.
We wrap the function \textbf{(+1)} in a monad so that everytime it is executed, we keep a record of its inputs and outputs, so we know that the component function mapped \textbf{[(1,2),(2,3),(3,4)]} to give an output from map of \textbf{[2,3,4]}.

Well now we have generated an example for the subexample generation function \textbf{example\_input = ([1,2,3],[2,3,4] ; example\_output = [(1,2),(2,3),(3,4)]}.
So, what if we try to run PBE on this? 
Interestingly, this is a slightly easier synthesis problem (than user-level synth) since we have the ability to generate as many examples as we want. 
There is also not as much of a time constraint since this is a preprocessing step, or "compile time cost", before the user starts actually running synthesis themselves. 
The other crazy thing is that we can apply the \textit{incompletely built} PBE engine recursively here, until we reach a synthesis problem that is easy to solve, then propagate that solution back up to the top level.

\section{User-level Synthesis}
We synthesizing the refinement types for examples we need to be careful to not generate the strongest possible refineent type.
Since example are inherently an underspecification, this can lead to problems.
As an example consider \textbf{filter}, for which all the examples a user provide may actually filter an element.
The strongest refinement type for this would  \textbf{xs:[a] -> {v:[a] | (len v) < (len xs)}} , while the refinement type for filter is correctly \textbf{filter :: (a -> Bool) -> xs:[a] -> {v:[a] | (len v) <= (len xs)}}.
We may need to deal with subtyping, but if we choose a limited and sensible pool of template refinement types to start with, we might be able to avoid that whole can of worms.  

\section{subexample Generation}
\begin{minted}{haskell}
testInputs = [( (>20), [10,20,30]
              , even, [6,7,8]
              , True, ['a','b','c'])]
filterExs = map filter testInputs
filter  :: (a -> Bool)
        -> xs:[a]
        -> {ys:[a] | len xs >= len ys}
map2    :: (a -> b)
        -> xs:[a]
        -> {ys:[a] | len xs < len ys}
\end{minted}

The subexample generation functions are synthesized from examples collected by wrapping the component function in a state monad to collect a record of its executions.
We now have a set of examples for top-level input, top-level output, and subexample.
We can recursively call PBE... This isn't the right way I think.


\begin{minted}{haskell}
p' p x = write x >> write (p x) >> return (p x)
filterSubs = map (filter p') testInputs
{- ([10,20,30] ~> [30] ~> [10 ~> False, 20 ~> False, 30 ~> True]
   ,[6,7,8] ~> [6,8] ~> [6 ~> True, 7 ~> False, 8 ~> True]
   ,['a','b'] ~> ['a','b'] ~> ['a' ~> True, 'b' ~> True] -}
   
filterGen :: [[a] ~> [a]] -> [(a ~> Bool)]
filterGen = synthesize filterSubs
mapGen :: [[a] ~> [b]] -> [(a ~> b)]
\end{minted}


The compilation of the synthesis engine is complete once the refinement types and subexample generation functions have been generated. We may use this engine for the runtime synthesis of the user. After the user provides the examples, the engine must choose a possible higher order function that fits those examples. To do this, we also synthesize refinement types for the given examples. Then, using the builtin type-checking, we easily determine that the appropriate higher order function is \textbf{filter}.

\begin{minted}{haskell}
exs :: [xs:[a] ~> {ys:[a] | len xs >= len ys}]
\end{minted}

The component function of filter, the predicate \textbf{p}, will be synthesized with programming by example, but we must first provide examples. Applying the subexample generator to the user provided examples yields a list of examples for each top-level example. Once the examples map primitives to primitives, we can do exhaustive search, or call an SMT solver probably.

\begin{minted}{haskell}
subExs = filterGen exs
{- [ 1 ~> False, 2 ~> False, 3 ~> True
   , 3 ~> True,  4 ~> True,  5 ~> True
   , 4 ~> True,  3 ~> True,  2 ~> False] -}
\end{minted}


As an example of what happens...
\begin{lstlisting}[language=haskell]
map :: (a -> b) -> [a] -> [a]
exs :: [Int] :-> [Int]

--component function must generalize
f :: Int -> Int

goodFxn1 :: Int  -> Int
goodFxn2 :: Int  -> a
goodFxn3 :: a    -> a
badFxn   :: Bool -> Bool
\end{lstlisting}


\begin{comment}
\subsubsection{Extending Liquid Haskell}\label{extLiqHask}
\markk{MARVIN - write something that makes more sense here}
LiquidHaskell does not support certain popular syntax extensions to Haskell, such as LambdaCase (TODO list others). 

In the spirit of this work, we wish to support as much user defined code as possible. To this end, we can extend the refinement type system by allowing refinement type inference on representative examples of a higher order function. Take the following code \markk{find something from an actual library on hackage.}

\begin{lstlisting}
{-# LANGAUGE LambdaCase #-}
fooMap :: (a -> b) -> [a] -> [b]
fooMap f = \case
  [] -> []
  l -> map f l
 \end{lstlisting}

In order to do this, we need to know the concrete type signature of higher order function when applied to the examples.

We generate and applying many examples with QuickCheck for each higher order function.
We then apply a similar refinement type inference strategy as above to these examples.
This lets us support a larger subset of the language, and, in theory refinement type inference for other languages too!

There are however repercussions to this approach. We are not guaranteed to generate a correct refinement type because we might not generate a fully representative examples. So we might prune away high order functions that are actually useful. 


\end{comment}






%this entire thing is false in fact, we could have mapIdInt :: (a->b) -> [Int]->[Int]
Since examples must be given as a concrete type, we can always specialize a our candidate higher order function using basic type unification\cite{typeUnif}. 
Additionally, given our requirement that all type variables are determined by the uncurried input from Section \ref{problem}, we will have specialized all type variables.
Without this assumption, it would be possible to have lingering polymorphism in the function signature (e.g. with \codeinline{exs::[Int]->[Int]), we could only conclude \codeinline{zipWith::(a->Int->Int)->[a]->[Int]->[Int]}).
By specializing the higher order function on the example type, we now know the concrete type of the entire component function.

An example of this would be trying to use \codeinline{negate::Int->Int} as a component for \codeinline{::a->a}


A \textit{dismantling procedure} prunes the first order function search space by using information from the top level examples, and the current state of synthesis.


\begin{lstlisting}
specializeOn :: Type -> Type -> Type
specializeOn concTy hoTy =
  replaceTysIn hoTy typeMap
  where
    typeMap = makeTypeMap 
                concTy 
                (lastAsFunType hoTy)
                Map.empty
\end{lstlisting}

For each higher-order function, we supply it with arguments until it is compatible with the type signature implied by the example set. In accordance with our problem definition in Section \ref{problem}, we can assume the final argument of the function is the input. Any other initial value types are satisfied by selecting from a pool of default values, and function types are satisfied by searching for first-order functions that would make the resulting signatures match. If it is not possible to find values that fit, the search moves on to the next higher-order function.
