\subsection{Soundness and Completeness}\label{sound}

It is clear that no function will be returned by the algorithm that does not fit the examples given, since functions are validated before being reported. 
Therefore, it is trivial to conclude that \ourTool/ is sound over the given examples.
Still, it is possible for the synthesis procedure to return a function that does not capture the user's intent - that is, as with any programming by examples system, \ourTool/ is not sound over the user intent.
Generally, this ambiguity can be resolved by the user supplying more examples to narrow the set of possible fitting functions.
However, depending on what the user is trying to synthesize, and which examples have been provided, it is possible for new examples to increase the internal search space. 
If, for example, a user gives only positive examples for a \texttt{filter}, the refinement type predicate discovery will assume that the lists do not change size, and will likely return \texttt{map id} as a result.

The completeness claim we might like to make is that over the solution space defined in Section \ref{problem}, we will always find a solution if it exists.
Since our space is finite, completeness can be made trivially true by replacing all instances of pruning with a zero ranking, so that our algorithm now is only a best-first enumerative search.
Because we make some decisions in pruning that removes potentially sound functions, such as using the \codeinline{noRType} tag in Section \ref{HORtypeInf} we trade this completeness for performance.
In Section \ref{sec:related}, we will discuss why, even if we had completeness, it should be sacrificed in future work.
%On the other hand, the set of functions that the algorithm can produce is fairly broad. It is able to search through the entire space of higher order functions that have been specialized with a first-order function, when considering the functions that are in scope. We will see in Section \ref{evaluation} how broad this space actually is. \markk{See Section \ref{solnSpace}}


\subsection{Performance}

\begin{table*}[t]
  \centering
  \begin{tabular}{|lllll|}
     & any (odd)        & 4.07   &  &  \\
    \hline

     & all (even)       & 1.93   &  &  \\
     \hline
    
     & takeWhile (odd)  & 8.96   &  &  \\
     \hline
    
     & map (not)        & 7.06   &  &  \\
     \hline
    
    or  & any (id)      & 3.72   &  &  \\
        & foldl1 (max)  & 4.37   &  &  \\
        & foldr1 (max)  & 4.45   &  &  \\
     \hline
    
    and & all (id)      & 2.36   &  &  \\
        & foldl1 (min)  & 4.53   &  &  \\
        & foldr1 (min)  & 4.57   &  &  \\
     \hline
    
    and & foldl  (add) (0)   & 1.68   &  &  \\
        & foldl' (add) (0)   & 7.56   &  &  \\
        & foldr  (add) (0)   & 13.95  &  &  \\
        & foldr' (add) (0)   & 19.94  &  &  \\
    \hline
    
        & mapBTree (id)      & 2.18   &  &  \\
    \hline

    & foldl (count) (0)  & 1.67   &  &  \\
    \hline

    &  concatMap (replicate (2))  & 1.82   &  &  \\
  \end{tabular}
  \caption{Benchmarks and Performance Measures}
  \label{tab:benchmarks}
\end{table*}

Since the standard library can be considered a relatively stable set of code, we could cache the refinement type inference to reduce the build time.

In Section \ref{HORtypeInf} we discuss using type matching and the \codeinline{noRType} tag to reduce the number of refinement type inferences we must make. 
Recall that even if both type have a measure (lists and trees), in general we have no guarantee that this is meaningful comparison.
Since \lhask/ is the largest cost to our system in the offline stage, removing refinement type inference in these ambiguous cases provides a large performance gain.
As an example, in processing the Haskell standard library \codeinline{base:Prelude}, there are 7 out of 30 cases of higher order functions that do not need to be checked against refinement types using this approach.

All reported data is generated on a Linux desktop machine with an Intel i5-3450 @ 3.10GHz and 8 Gb of ram

\subsection{Example Generation}\label{languageSupport}

We have tried to avoid code analysis at every stage of this paper.
However there are two points where this has fallen short. 
First, we must parse a file to extract the name and type information of every top level identifier. 
Second, using \lhask/ as a blackbox means that we are limited by \lhask/'s ability to deduce refinement types over functions. 
Our eventual goal is to create a system that can be easily ported across functional languages. 
Luckily, the first code dependency is small enough to handle with ease in most typed languages (the grammar of a type signature is relatively small). However \lhask/ is a powerful tool that would be difficult to recreate in another language. 

To this end, we can extend the refinement type system by allowing refinement type inference on representative examples of a higher order function.
We do not need to identify a particular component function since we are only interested in size based refinement types.
We then apply a similar refinement type inference strategy as in Listing \ref{exRTypeGen} to these examples.

Our current example generation tool uses QuickCheck to generate and apply many examples for higher order function Haskell~\ref{quickcheck}.
Of course, since \lhask/ supports so much of Haskell, this is not practically useful for us, but provides a prototype as a proof of concept.
Imagining that we could not find a refinement type directly on map, we might use examples to infer a refinement type. Take the following code:

\begin{lstlisting}
map :: (a -> b) -> [a] -> [b]
map f [] = [] 
map f x:xs = f x : map f xs

mapExs = [[1,2,3] :-> [4,2,8]]
\end{lstlisting}

There are however repercussions to this approach. We are not guaranteed to generate a correct refinement type because we might not generate a fully representative examples. It then seems it is possible prune away many high order functions that are actually useful, but the full repercussions of this is outside the scope of this paper.


